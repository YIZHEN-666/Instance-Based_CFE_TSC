{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN Black-box Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.4.1\n",
      "Eager execution enabled:  False\n",
      "TF version:  2.4.1\n",
      "Eager execution enabled:  False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "tf.get_logger().setLevel(40) # suppress deprecation messages\n",
    "tf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv1D, GlobalAveragePooling1D, BatchNormalization, Conv2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.backend import function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from counterfactual_utils import label_encoder, ucr_data_loader\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # False\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Architecture\n",
    "\n",
    "This architecture was originally proposed by Wang et al and the implementation closely follows the code provided by Fawaz et al in a fantastic review paper on DNNs for Time series classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_FCN:\n",
    "    \n",
    "    def __init__(self, output_directory, input_shape, nb_classes, dataset_name, verbose=False,build=True):\n",
    "        self.output_directory = output_directory\n",
    "\n",
    "        \n",
    "        if build == True:\n",
    "            self.model = self.build_model(input_shape, nb_classes)\n",
    "            if(verbose==True):\n",
    "                self.model.summary()\n",
    "            self.verbose = verbose\n",
    "            self.model.save_weights(str(dataset_name) +'_model_init.hdf5')\n",
    "        return\n",
    "\n",
    "    def build_model(self, input_shape, nb_classes):\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "        conv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding='same')(input_layer)\n",
    "        conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "        conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "        conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
    "        conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "        conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "        conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
    "        conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "        conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "        gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, \n",
    "            min_lr=0.0001)\n",
    "\n",
    "        file_path = str(dataset_name) +'_best_model.hdf5'\n",
    "\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss', \n",
    "            save_best_only=True)\n",
    "\n",
    "        self.callbacks = [reduce_lr,model_checkpoint]\n",
    "\n",
    "        return model \n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "         \n",
    "        batch_size = 16\n",
    "        nb_epochs = 2000\n",
    "\n",
    "        mini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n",
    "        \n",
    "\n",
    "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
    "            verbose=self.verbose, callbacks=self.callbacks)\n",
    "\n",
    "        self.model.save(str(dataset_name) +'_last_model.hdf5')\n",
    "\n",
    "        model = keras.models.load_model(str(dataset_name) +'_best_model.hdf5')\n",
    "\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        model_path = str(dataset_name) + '_best_model.hdf5'\n",
    "        model = keras.models.load_model(model_path)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset in ['ecg200', 'gunpoint', 'coffee', 'chinatown', 'CBF']:\n",
    "\n",
    "#    X_train, y_train, X_test, y_test = ucr_data_loader(str(dataset))\n",
    "#    y_train, y_test = label_encoder(y_train, y_test)\n",
    "    \n",
    "#    input_shape = X_train.shape[1:]\n",
    "#    nb_classes = len(np.unique(np.concatenate([y_train,y_test])))\n",
    "#    one_hot = to_categorical(y_train)\n",
    "#    dataset_name = str(dataset)\n",
    "    \n",
    "    \n",
    "#    fcn = Classifier_FCN(output_directory=os.getcwd(), input_shape=input_shape, nb_classes=nb_classes, dataset_name=dataset_name)\n",
    "#    fcn.build_model(input_shape=input_shape, nb_classes=nb_classes)\n",
    "#    fcn.fit(X_train, to_categorical(y_train))\n",
    "#    fcn.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
